{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqNzseYaiZmpT4OQLxEuhe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["CUSTOM IMPLEMENTATION\n"],"metadata":{"id":"RhNcgdeV64fg"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EM2acPy7_GWQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745267289432,"user_tz":240,"elapsed":22436,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"aa0a7b17-0f66-442a-fed7-8fdb0179c219"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ResNet Model Evaluation on ImageNet Test Set Sample with Custom INT8 Quantization\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import random\n","import os\n","from torch.utils.data import Subset, DataLoader, Dataset\n","from PIL import Image\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# To test on CPU also.\n","quantization_device = torch.device(\"cpu\")\n","\n","# This is the standard preprocessing for models pretrained on ImageNet\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Path to the extracted ImageNet data\n","imagenet_path = \"/content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/\"\n","print(f\"Loading from: {imagenet_path}\")\n","\n","all_image_files = []\n","class_folders = [f for f in os.listdir(imagenet_path) if os.path.isdir(os.path.join(imagenet_path, f))]\n","print(f\"Found {len(class_folders)} class folders (00000 to 00108)\")\n","\n","for folder in class_folders:\n","    folder_path = os.path.join(imagenet_path, folder)\n","    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n","    all_image_files.extend(files)\n","\n","print(f\"Found a total of {len(all_image_files)} images\")\n","\n","# Randomly sample 50 images\n","num_samples = 50\n","if len(all_image_files) > num_samples:\n","    random.shuffle(all_image_files)\n","    sampled_images = all_image_files[:num_samples]\n","else:\n","    sampled_images = all_image_files\n","\n","print(f\"Randomly sampled {len(sampled_images)} images for testing\")\n","\n","# Create a simple mapping from folder name to class index\n","folder_to_idx = {folder: idx for idx, folder in enumerate(sorted(class_folders))}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuAhz8wPnHFB","executionInfo":{"status":"ok","timestamp":1745267331051,"user_tz":240,"elapsed":41608,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"dccf2891-3c91-4a0e-f32f-daf6dede3fbd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Loading from: /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/\n","Found 109 class folders (00000 to 00108)\n","Found a total of 5450 images\n","Randomly sampled 50 images for testing\n"]}]},{"cell_type":"code","source":["# Custom dataset for the sampled images\n","class SampledImageNetDataset(Dataset):\n","    def __init__(self, image_files, transform=None):\n","        self.image_files = image_files\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_files[idx]\n","\n","        # Get the class folder from the path\n","        folder_name = os.path.basename(os.path.dirname(img_path))\n","        class_idx = folder_to_idx[folder_name]\n","\n","        # Load and transform the image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, class_idx\n","\n","# Create the dataset and dataloader\n","sample_dataset = SampledImageNetDataset(sampled_images, transform=preprocess)\n","sample_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False)\n","\n","print(f\"Created dataset with {len(sample_dataset)} images\")\n","print(f\"Number of classes represented: {len(set(folder_to_idx.values()))}\")\n","\n","# Create class mapping for visualization (folder name to index)\n","idx_to_class = {v: k for k, v in folder_to_idx.items()}\n","\n","# Print a few examples\n","print(\"\\nSample images:\")\n","for i in range(min(5, len(sampled_images))):\n","    img_path = sampled_images[i]\n","    folder = os.path.basename(os.path.dirname(img_path))\n","    print(f\"{i+1}. {img_path} (Class: {folder})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDinEPY_7mFT","executionInfo":{"status":"ok","timestamp":1745267331053,"user_tz":240,"elapsed":69,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"d5359414-2bd8-478d-83d6-71f7bf97c2f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Created dataset with 50 images\n","Number of classes represented: 109\n","\n","Sample images:\n","1. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00074/324421257216414.jpg (Class: 00074)\n","2. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00040/799694495669414.jpg (Class: 00040)\n","3. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00035/08141709225444.jpg (Class: 00035)\n","4. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00039/899020713517552.jpg (Class: 00039)\n","5. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00067/291797602056391.jpg (Class: 00067)\n"]}]},{"cell_type":"code","source":["# Custom INT8 quantization functions\n","def clamp(params_q: np.array, lower_bound: int, upper_bound: int) -> np.array:\n","    \"\"\"Clamp values to specified bounds\"\"\"\n","    params_q[params_q < lower_bound] = lower_bound\n","    params_q[params_q > upper_bound] = upper_bound\n","    return params_q\n","\n","def asymmetric_quantization(params: np.array, bits: int) -> tuple:\n","    \"\"\"Quantize parameters using min-max asymmetric quantization\"\"\"\n","    alpha = np.max(params)\n","    beta = np.min(params)\n","    scale = (alpha - beta) / (2**bits-1)\n","    zero = -1*np.round(beta / scale)\n","    lower_bound, upper_bound = 0, 2**bits-1\n","    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)\n","    return quantized, scale, zero\n","\n","def asymmetric_quantization_percentile(params: np.array, bits: int, percentile: float = 99.99) -> tuple:\n","    \"\"\"Quantize parameters using percentile-based asymmetric quantization\"\"\"\n","    # find the percentile value\n","    alpha = np.percentile(params, percentile)\n","    beta = np.percentile(params, 100-percentile)\n","    scale = (alpha - beta) / (2**bits-1)\n","    zero = -1*np.round(beta / scale)\n","    lower_bound, upper_bound = 0, 2**bits-1\n","    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)\n","    return quantized, scale, zero\n","\n","def asymmetric_dequantize(params_q: np.array, scale: float, zero: int) -> np.array:\n","    \"\"\"Dequantize parameters using scale and zero point\"\"\"\n","    return (params_q - zero) * scale\n","\n","def quantization_error(params: np.array, params_q: np.array):\n","    \"\"\"Calculate the MSE between original and quantized parameters\"\"\"\n","    return np.mean((params - params_q)**2)\n","\n","# Custom quantized Conv2d module\n","class QuantizedConv2d(torch.nn.Module):\n","    def __init__(self, conv_module, bits=8, percentile=False, percentile_value=99.99):\n","        super(QuantizedConv2d, self).__init__()\n","\n","        # Store original module properties\n","        self.in_channels = conv_module.in_channels\n","        self.out_channels = conv_module.out_channels\n","        self.kernel_size = conv_module.kernel_size\n","        self.stride = conv_module.stride\n","        self.padding = conv_module.padding\n","        self.dilation = conv_module.dilation\n","        self.groups = conv_module.groups\n","        self.bias = conv_module.bias is not None\n","\n","        # Quantization parameters\n","        self.bits = bits\n","        self.percentile = percentile\n","        self.percentile_value = percentile_value\n","\n","        # Get weight data as numpy array\n","        weight_data = conv_module.weight.data.cpu().numpy()\n","\n","        # Quantize weights\n","        if percentile:\n","            self.weight_q, self.weight_scale, self.weight_zero = \\\n","                asymmetric_quantization_percentile(weight_data, bits, percentile_value)\n","        else:\n","            self.weight_q, self.weight_scale, self.weight_zero = \\\n","                asymmetric_quantization(weight_data, bits)\n","\n","        # Store weight shape for later reshaping\n","        self.weight_shape = weight_data.shape\n","\n","        # If bias exists, quantize it as well\n","        if self.bias:\n","            bias_data = conv_module.bias.data.cpu().numpy()\n","            if percentile:\n","                self.bias_q, self.bias_scale, self.bias_zero = \\\n","                    asymmetric_quantization_percentile(bias_data, bits, percentile_value)\n","            else:\n","                self.bias_q, self.bias_scale, self.bias_zero = \\\n","                    asymmetric_quantization(bias_data, bits)\n","            self.bias_shape = bias_data.shape\n","\n","        # Pre-compute dequantized weights for efficiency\n","        # This avoids repeated dequantization during inference\n","        weight_dequant = asymmetric_dequantize(self.weight_q, self.weight_scale, self.weight_zero)\n","        self.weight_dequant = torch.from_numpy(weight_dequant.reshape(self.weight_shape)).float()\n","\n","        if self.bias:\n","            bias_dequant = asymmetric_dequantize(self.bias_q, self.bias_scale, self.bias_zero)\n","            self.bias_dequant = torch.from_numpy(bias_dequant).float()\n","        else:\n","            self.bias_dequant = None\n","\n","    def forward(self, x):\n","        # Use pre-computed dequantized weights and bias\n","        # This is much faster than dequantizing on every forward pass\n","        return torch.nn.functional.conv2d(\n","            x, self.weight_dequant.to(x.device),\n","            self.bias_dequant.to(x.device) if self.bias_dequant is not None else None,\n","            stride=self.stride,\n","            padding=self.padding,\n","            dilation=self.dilation,\n","            groups=self.groups\n","        )\n","\n","# Custom quantized Linear module\n","class QuantizedLinear(torch.nn.Module):\n","    def __init__(self, linear_module, bits=8, percentile=False, percentile_value=99.99):\n","        super(QuantizedLinear, self).__init__()\n","\n","        # Store original module properties\n","        self.in_features = linear_module.in_features\n","        self.out_features = linear_module.out_features\n","        self.bias = linear_module.bias is not None\n","\n","        # Quantization parameters\n","        self.bits = bits\n","        self.percentile = percentile\n","        self.percentile_value = percentile_value\n","\n","        # Get weight data as numpy array\n","        weight_data = linear_module.weight.data.cpu().numpy()\n","\n","        # Quantize weights\n","        if percentile:\n","            self.weight_q, self.weight_scale, self.weight_zero = \\\n","                asymmetric_quantization_percentile(weight_data, bits, percentile_value)\n","        else:\n","            self.weight_q, self.weight_scale, self.weight_zero = \\\n","                asymmetric_quantization(weight_data, bits)\n","\n","        # Store weight shape for later reshaping\n","        self.weight_shape = weight_data.shape\n","\n","        # If bias exists, quantize it as well\n","        if self.bias:\n","            bias_data = linear_module.bias.data.cpu().numpy()\n","            if percentile:\n","                self.bias_q, self.bias_scale, self.bias_zero = \\\n","                    asymmetric_quantization_percentile(bias_data, bits, percentile_value)\n","            else:\n","                self.bias_q, self.bias_scale, self.bias_zero = \\\n","                    asymmetric_quantization(bias_data, bits)\n","            self.bias_shape = bias_data.shape\n","\n","        # Pre-compute dequantized weights and bias for efficiency\n","        weight_dequant = asymmetric_dequantize(self.weight_q, self.weight_scale, self.weight_zero)\n","        self.weight_dequant = torch.from_numpy(weight_dequant.reshape(self.weight_shape)).float()\n","\n","        if self.bias:\n","            bias_dequant = asymmetric_dequantize(self.bias_q, self.bias_scale, self.bias_zero)\n","            self.bias_dequant = torch.from_numpy(bias_dequant).float()\n","        else:\n","            self.bias_dequant = None\n","\n","    def forward(self, x):\n","        # Use pre-computed dequantized weights and bias\n","        return torch.nn.functional.linear(\n","            x,\n","            self.weight_dequant.to(x.device),\n","            self.bias_dequant.to(x.device) if self.bias_dequant is not None else None\n","        )\n","\n","# Function to recursively replace modules with quantized versions\n","def quantize_model(model, bits=8, use_percentile=False, percentile_value=99.99):\n","    \"\"\"Replace Conv2d and Linear layers with quantized versions\"\"\"\n","    for name, module in model.named_children():\n","        if len(list(module.children())) > 0:\n","            # Recursively quantize submodules if they exist\n","            quantize_model(module, bits, use_percentile, percentile_value)\n","        else:\n","            # Quantize if module is Conv2d or Linear\n","            if isinstance(module, torch.nn.Conv2d):\n","                setattr(model, name, QuantizedConv2d(module, bits, use_percentile, percentile_value))\n","            elif isinstance(module, torch.nn.Linear):\n","                setattr(model, name, QuantizedLinear(module, bits, use_percentile, percentile_value))\n","\n","    return model\n","\n"],"metadata":{"id":"SVrJ3rdF-Mko","executionInfo":{"status":"ok","timestamp":1745267331069,"user_tz":240,"elapsed":50,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Function to load ResNet-18 model\n","def load_model(quantize=False, quantization_type='standard', bits=8):\n","    # Load the PyTorch model\n","    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","    if not quantize:\n","        # Regular non-quantized model\n","        model = model.to(device)\n","        model.eval()\n","        return model\n","\n","    # For quantization, work with the model on CPU\n","    model = model.to('cpu')\n","    model.eval()\n","\n","    if quantization_type == 'standard':\n","        # Use standard min-max quantization\n","        quantized_model = quantize_model(model, bits=bits, use_percentile=False)\n","        return quantized_model\n","\n","    elif quantization_type == 'percentile':\n","        # Use percentile-based quantization\n","        quantized_model = quantize_model(model, bits=bits, use_percentile=True, percentile_value=99.99)\n","        return quantized_model\n","\n","    # Return original model if no valid quantization type\n","    return model\n","\n","# Function to measure FPS\n","def measure_fps(model, input_tensor, is_quantized=False):\n","    # Always use the same device for fair comparison\n","    # Using CPU for all models since the quantized implementation\n","    # doesn't have GPU acceleration anyway\n","    input_tensor = input_tensor.to('cpu')\n","    model = model.to('cpu')\n","\n","    # Warm-up run (multiple iterations to ensure any caching effects are stabilized)\n","    with torch.no_grad():\n","        for _ in range(5):\n","            model(input_tensor)\n","\n","    # Actual timing - increased number of runs for better statistics\n","    start_time = time.time()\n","    num_runs = 20\n","    with torch.no_grad():\n","        for _ in range(num_runs):\n","            model(input_tensor)\n","\n","    end_time = time.time()\n","    time_per_image = (end_time - start_time) / num_runs\n","    fps = 1.0 / time_per_image\n","\n","    return fps\n","\n","# Function to evaluate model on the test set\n","def evaluate_model(model, data_loader, is_quantized=False):\n","    if is_quantized:\n","        model = model.to('cpu')\n","        eval_device = 'cpu'\n","    else:\n","        model = model.to(device)\n","        eval_device = device\n","\n","    correct = 0\n","    total = 0\n","\n","    # For top-1 and top-5 accuracy\n","    top1_correct = 0\n","    top5_correct = 0\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(eval_device), labels.to(eval_device)\n","            outputs = model(images)\n","\n","            # Top-1 accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            top1_correct += (predicted == labels).sum().item()\n","\n","            # Top-5 accuracy\n","            _, top5_preds = torch.topk(outputs, 5, dim=1)\n","            for i in range(labels.size(0)):\n","                if labels[i] in top5_preds[i]:\n","                    top5_correct += 1\n","\n","            total += labels.size(0)\n","\n","    top1_accuracy = 100 * top1_correct / total\n","    top5_accuracy = 100 * top5_correct / total\n","\n","    return top1_accuracy, top5_accuracy"],"metadata":{"id":"p6tGAi87_NvT","executionInfo":{"status":"ok","timestamp":1745267336675,"user_tz":240,"elapsed":40,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# A batch-wise activation quantization wrapper for performance simulation\n","class ActivationQuantizer(torch.nn.Module):\n","    def __init__(self, wrapped_model, bits=8, use_percentile=False, percentile_value=99.99):\n","        super(ActivationQuantizer, self).__init__()\n","        self.wrapped_model = wrapped_model\n","        self.bits = bits\n","        self.use_percentile = use_percentile\n","        self.percentile_value = percentile_value\n","\n","        # Store activation scales for each layer (to be calibrated)\n","        self.activation_scales = {}\n","        self.activation_zeros = {}\n","        self.calibrated = False\n","\n","    def calibrate(self, dataloader, num_batches=10):\n","        \"\"\"Calibrate activation quantization parameters using sample data\"\"\"\n","        self.wrapped_model.eval()\n","        activation_ranges = {}\n","\n","        # Register forward hooks to capture activations\n","        hooks = []\n","\n","        def get_activation_hook(name):\n","            def hook(module, input, output):\n","                # Store activations for this layer\n","                if name not in activation_ranges:\n","                    activation_ranges[name] = {\"min\": float('inf'), \"max\": float('-inf')}\n","\n","                # Update min/max values\n","                act_min = output.min().item()\n","                act_max = output.max().item()\n","\n","                activation_ranges[name][\"min\"] = min(activation_ranges[name][\"min\"], act_min)\n","                activation_ranges[name][\"max\"] = max(activation_ranges[name][\"max\"], act_max)\n","            return hook\n","\n","        # Register hooks on all Conv2d and Linear layers\n","        for name, module in self.wrapped_model.named_modules():\n","            if isinstance(module, (QuantizedConv2d, QuantizedLinear)):\n","                hooks.append(module.register_forward_hook(get_activation_hook(name)))\n","\n","        # Run calibration data through the model\n","        with torch.no_grad():\n","            for batch_idx, (inputs, _) in enumerate(dataloader):\n","                if batch_idx >= num_batches:\n","                    break\n","                inputs = inputs.to('cpu')\n","                _ = self.wrapped_model(inputs)\n","\n","        # Remove the hooks\n","        for hook in hooks:\n","            hook.remove()\n","\n","        # Compute quantization parameters for each layer\n","        for name, ranges in activation_ranges.items():\n","            alpha = ranges[\"max\"]\n","            beta = ranges[\"min\"]\n","\n","            if self.use_percentile:\n","                # For percentile-based, more complex statistics are needed\n","                # simplifying here and just using the min/max\n","                pass\n","\n","            scale = (alpha - beta) / (2**self.bits - 1)\n","            zero = -1 * np.round(beta / scale) if scale != 0 else 0\n","\n","            self.activation_scales[name] = scale\n","            self.activation_zeros[name] = zero\n","\n","        self.calibrated = True\n","        print(f\"Calibrated activation quantization for {len(activation_ranges)} layers\")\n","\n","    def forward(self, x):\n","        # Forward through the wrapped model\n","        return self.wrapped_model(x)"],"metadata":{"id":"ZGWuWnUB-pog","executionInfo":{"status":"ok","timestamp":1745267343924,"user_tz":240,"elapsed":39,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Function to evaluate models\n","def evaluate_batched(model, dataloader, is_quantized=False, batch_size=8):\n","    \"\"\"Process data in batches for more efficient evaluation\"\"\"\n","    if is_quantized:\n","        model = model.to('cpu')\n","        eval_device = 'cpu'\n","    else:\n","        model = model.to(device)\n","        eval_device = device\n","\n","    # Create a new dataloader with the desired batch size\n","    batch_dataloader = DataLoader(\n","        dataloader.dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=0\n","    )\n","\n","    top1_correct = 0\n","    top5_correct = 0\n","    total = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in batch_dataloader:\n","            images, labels = images.to(eval_device), labels.to(eval_device)\n","            outputs = model(images)\n","\n","            # Top-1 accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            top1_correct += (predicted == labels).sum().item()\n","\n","            # Top-5 accuracy\n","            _, top5_preds = torch.topk(outputs, 5, dim=1)\n","            for i in range(labels.size(0)):\n","                if labels[i] in top5_preds[i]:\n","                    top5_correct += 1\n","\n","            total += labels.size(0)\n","\n","    top1_accuracy = 100 * top1_correct / total\n","    top5_accuracy = 100 * top5_correct / total\n","\n","    return top1_accuracy, top5_accuracy\n","\n","# Function to visualize predictions\n","def visualize_predictions(model_obj, dataset, loader, num_samples=5, is_quantized=False):\n","    if is_quantized:\n","        model_obj = model_obj.to('cpu')\n","        vis_device = 'cpu'\n","    else:\n","        model_obj = model_obj.to(device)\n","        vis_device = device\n","\n","    model_obj.eval()\n","    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n","\n","    with torch.no_grad():\n","        for i, (images, labels) in enumerate(loader):\n","            if i >= num_samples:\n","                break\n","\n","            images, labels = images.to(vis_device), labels.to(vis_device)\n","            outputs = model_obj(images)\n","\n","            # Get predicted class\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Get class names (folder names)\n","            true_class_idx = labels.item()\n","            pred_class_idx = predicted.item()\n","\n","            true_class = idx_to_class[true_class_idx]\n","            pred_class = idx_to_class.get(pred_class_idx, f\"Unknown ({pred_class_idx})\")\n","\n","            # Get image\n","            img = images[0].cpu().numpy().transpose((1, 2, 0))\n","            # Denormalize\n","            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n","            img = np.clip(img, 0, 1)\n","\n","            # Plot\n","            axes[i].imshow(img)\n","            axes[i].set_title(f\"True: {true_class}\\nPred: {pred_class}\")\n","            axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig('sample_predictions_custom_quantized.png')\n","    plt.show()\n","\n","# Calculate model size\n","def calculate_model_size(model, is_quantized=False, bits=32):\n","    if is_quantized:\n","        # For quantized models, consider bit width\n","        size_in_bytes = 0\n","        for name, module in model.named_modules():\n","            if hasattr(module, 'weight_q') and hasattr(module, 'bits'):\n","                # Count weight parameters\n","                size_in_bytes += np.prod(module.weight_q.shape) * module.bits / 8\n","\n","                # Count bias parameters if they exist\n","                if hasattr(module, 'bias_q'):\n","                    size_in_bytes += np.prod(module.bias_q.shape) * module.bits / 8\n","            elif hasattr(module, 'weight'):\n","                # For non-quantized parts within the model\n","                size_in_bytes += module.weight.numel() * 4  # float32 = 4 bytes\n","                if hasattr(module, 'bias') and module.bias is not None:\n","                    size_in_bytes += module.bias.numel() * 4\n","    else:\n","        # For non-quantized models, count parameters assuming float32\n","        size_in_bytes = sum(p.numel() for p in model.parameters()) * 4  # float32 = 4 bytes\n","\n","    return size_in_bytes / (1024 * 1024)  # Convert to MB"],"metadata":{"id":"Ufp0Wgat-UI5","executionInfo":{"status":"ok","timestamp":1745267348301,"user_tz":240,"elapsed":13,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Function to run multiple evaluations and calculate averages\n","def run_multiple_evaluations(num_runs=5):\n","    # Store results for each run\n","    run_results = {\n","        'models': ['ResNet18 Original', 'ResNet18 INT8 Min-Max', 'ResNet18 INT8 Percentile'],\n","        'runs': [],\n","        'avg_top1_accuracy': [],\n","        'avg_top5_accuracy': [],\n","        'avg_model_size_mb': [],\n","        'avg_fps': [],  # Add this line to initialize the avg_fps list\n","    }\n","    # Create dummy input for FPS measurement\n","    dummy_input = torch.randn(1, 3, 224, 224)\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Starting Run {run+1}/{num_runs} ---\")\n","        run_data = {\n","            'top1_accuracy': [],\n","            'top5_accuracy': [],\n","            'model_size_mb': [],\n","            'fps': []\n","        }\n","\n","        # Evaluate original PyTorch ResNet-18\n","        print(f\"\\nRun {run+1}: Evaluating original PyTorch ResNet-18...\")\n","        resnet18_original = load_model(quantize=False)\n","\n","        model_size_original = calculate_model_size(resnet18_original)\n","        fps_original = measure_fps(resnet18_original, dummy_input, is_quantized=False)\n","        top1_acc_original, top5_acc_original = evaluate_model(resnet18_original, sample_loader, is_quantized=False)\n","\n","        run_data['top1_accuracy'].append(top1_acc_original)\n","        run_data['top5_accuracy'].append(top5_acc_original)\n","\n","        run_data['model_size_mb'].append(model_size_original)\n","        run_data['fps'].append(fps_original)\n","\n","        # Evaluate standard INT8 quantized model\n","        print(f\"\\nRun {run+1}: Evaluating ResNet-18 with Custom INT8 Quantization (Min-Max)...\")\n","        resnet18_int8_standard = load_model(quantize=True, quantization_type='standard', bits=8)\n","\n","        # Apply activation quantization wrapper and calibrate\n","        act_quantized_model = ActivationQuantizer(resnet18_int8_standard, bits=8, use_percentile=False)\n","        act_quantized_model.calibrate(DataLoader(sample_dataset, batch_size=2, shuffle=False))\n","\n","        model_size_int8_standard = calculate_model_size(resnet18_int8_standard, is_quantized=True, bits=8)\n","        fps_int8_standard = measure_fps(act_quantized_model, dummy_input, is_quantized=True)\n","        top1_acc_int8_standard, top5_acc_int8_standard = evaluate_batched(\n","            act_quantized_model, sample_loader, is_quantized=True, batch_size=4\n","        )\n","\n","        run_data['top1_accuracy'].append(top1_acc_int8_standard)\n","        run_data['top5_accuracy'].append(top5_acc_int8_standard)\n","        run_data['model_size_mb'].append(model_size_int8_standard)\n","        run_data['fps'].append(fps_int8_standard)\n","\n","        # Evaluate percentile INT8 quantized model\n","        print(f\"\\nRun {run+1}: Evaluating ResNet-18 with Custom INT8 Quantization (Percentile)...\")\n","        resnet18_int8_percentile = load_model(quantize=True, quantization_type='percentile', bits=8)\n","\n","        # Apply activation quantization wrapper and calibrate\n","        act_quantized_model_percentile = ActivationQuantizer(resnet18_int8_percentile, bits=8, use_percentile=True, percentile_value=99.99)\n","        act_quantized_model_percentile.calibrate(DataLoader(sample_dataset, batch_size=2, shuffle=False))\n","\n","        model_size_int8_percentile = calculate_model_size(resnet18_int8_percentile, is_quantized=True, bits=8)\n","        fps_int8_percentile = measure_fps(act_quantized_model_percentile, dummy_input, is_quantized=True)\n","        top1_acc_int8_percentile, top5_acc_int8_percentile = evaluate_batched(\n","            act_quantized_model_percentile, sample_loader, is_quantized=True, batch_size=4\n","        )\n","\n","        run_data['top1_accuracy'].append(top1_acc_int8_percentile)\n","        run_data['top5_accuracy'].append(top5_acc_int8_percentile)\n","\n","        run_data['model_size_mb'].append(model_size_int8_percentile)\n","        run_data['fps'].append(fps_int8_percentile)\n","\n","        # Store this run's data\n","        run_results['runs'].append(run_data)\n","\n","        print(f\"\\n--- Run {run+1} Summary ---\")\n","        print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'Size (MB)':<12} {'FPS':<10}\")\n","        print(\"-\" * 85)\n","\n","        for i, model_name in enumerate(run_results['models']):\n","            print(f\"{model_name:<30} {run_data['top1_accuracy'][i]:<15.2f} {run_data['top5_accuracy'][i]:<15.2f} {run_data['model_size_mb'][i]:<12.2f} {run_data['fps'][i]:<10.2f}\")\n","\n","    # Calculate averages across all runs\n","    for metric in ['top1_accuracy', 'top5_accuracy', 'model_size_mb', 'fps']:\n","        for model_idx in range(len(run_results['models'])):\n","            values = [run_results['runs'][run_idx][metric][model_idx] for run_idx in range(num_runs)]\n","            run_results[f'avg_{metric}'].append(np.mean(values))\n","\n","    # Calculate improvement/loss metrics\n","    original_model_idx = 0  # Index of the original model in results\n","\n","    # Calculate FPS improvement for each quantized model compared to original\n","    fps_improvements = []\n","    for i in range(1, len(run_results['models'])):\n","        fps_improvement = (run_results['avg_fps'][i] / run_results['avg_fps'][original_model_idx] - 1) * 100\n","        fps_improvements.append(fps_improvement)\n","\n","    # Calculate accuracy loss for each quantized model compared to original\n","    top1_acc_losses = []\n","    top5_acc_losses = []\n","    for i in range(1, len(run_results['models'])):\n","        top1_loss = run_results['avg_top1_accuracy'][original_model_idx] - run_results['avg_top1_accuracy'][i]\n","        top5_loss = run_results['avg_top5_accuracy'][original_model_idx] - run_results['avg_top5_accuracy'][i]\n","        top1_acc_losses.append(top1_loss)\n","        top5_acc_losses.append(top5_loss)\n","\n","    # Calculate model size reduction\n","    size_reductions = []\n","    for i in range(1, len(run_results['models'])):\n","        size_reduction = (1 - run_results['avg_model_size_mb'][i] / run_results['avg_model_size_mb'][original_model_idx]) * 100\n","        size_reductions.append(size_reduction)\n","\n","    # Print final average results\n","    print(\"\\n\\n====== FINAL RESULTS (AVERAGED OVER 5 RUNS) ======\")\n","    print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'Size (MB)':<12} {'FPS':<10}\")\n","    print(\"-\" * 85)\n","\n","    for i, model_name in enumerate(run_results['models']):\n","        print(f\"{model_name:<30} {run_results['avg_top1_accuracy'][i]:<15.2f} {run_results['avg_top5_accuracy'][i]:<15.2f} {run_results['avg_model_size_mb'][i]:<12.2f} {run_results['avg_fps'][i]:<10.2f}\")\n","\n","    # Print improvements/losses\n","    print(\"\\n====== IMPROVEMENTS AND LOSSES ======\")\n","    for i, model_name in enumerate(run_results['models'][1:], start=1):\n","        print(f\"\\n{model_name} compared to Original:\")\n","        print(f\"  FPS Improvement: {fps_improvements[i-1]:+.2f}%\")\n","        print(f\"  Top-1 Accuracy Loss: {top1_acc_losses[i-1]:+.2f}%\")\n","        print(f\"  Top-5 Accuracy Loss: {top5_acc_losses[i-1]:+.2f}%\")\n","        print(f\"  Model Size Reduction: {size_reductions[i-1]:.2f}%\")\n","\n","    # Plot the results\n","    plot_comparison_results(run_results)\n","\n","    return run_results"],"metadata":{"id":"4yFzYnFy8d9u","executionInfo":{"status":"ok","timestamp":1745267773548,"user_tz":240,"elapsed":7,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def plot_comparison_results(results):\n","    # Create a figure with subplots\n","    fig, axs = plt.subplots(1, 3, figsize=(18, 8))\n","\n","    # Get the data\n","    models = results['models']\n","    avg_top1 = results['avg_top1_accuracy']\n","    avg_top5 = results['avg_top5_accuracy']\n","    avg_size = results['avg_model_size_mb']\n","    avg_fps = results['avg_fps']\n","\n","    # Accuracy plot (combining top-1 and top-5)\n","    bar_width = 0.35\n","    x = np.arange(len(models))\n","    axs[0].bar(x - bar_width/2, avg_top1, bar_width, color='skyblue', label='Top-1')\n","    axs[0].bar(x + bar_width/2, avg_top5, bar_width, color='lightgreen', label='Top-5')\n","    axs[0].set_xlabel('Model')\n","    axs[0].set_ylabel('Accuracy (%)')\n","    axs[0].set_title('Average Test Accuracy')\n","    axs[0].set_xticks(x)\n","    axs[0].set_xticklabels(models, rotation=15, ha='right')\n","\n","    # Set y-axis limits based on data - adjust for accuracy plot\n","\n","    min_acc = min(min(avg_top1), min(avg_top5)) - 5\n","    max_acc = max(max(avg_top1), max(avg_top5)) + 5\n","\n","    min_acc = max(0, min_acc)\n","    max_acc = min(100, max_acc)\n","    axs[0].set_ylim([min_acc, max_acc])\n","\n","    axs[0].legend()\n","    for i, v in enumerate(avg_top1):\n","        axs[0].text(i - bar_width/2, v + 1, f\"{v:.2f}%\", ha='center')\n","    for i, v in enumerate(avg_top5):\n","        axs[0].text(i + bar_width/2, v + 1, f\"{v:.2f}%\", ha='center')\n","\n","    # Model size plot - Use numeric x positions for consistency\n","    axs[1].bar(x, avg_size, color='salmon')\n","    axs[1].set_xlabel('Model')\n","    axs[1].set_ylabel('Size (MB)')\n","    axs[1].set_title('Average Model Size (MB)')\n","    axs[1].set_xticks(x)\n","    axs[1].set_xticklabels(models, rotation=15, ha='right')\n","\n","    # Set y-axis limits based on data for model size\n","    min_size = 0\n","    max_size = max(avg_size) * 1.2  # Add 20% headroom\n","    axs[1].set_ylim([min_size, max_size])\n","\n","    for i, v in enumerate(avg_size):\n","        axs[1].text(i, v + (max_size - min_size) * 0.02, f\"{v:.2f}MB\", ha='center')\n","\n","    # FPS plot - Use numeric x positions for consistency\n","    axs[2].bar(x, avg_fps, color='gold')\n","    axs[2].set_xlabel('Model')\n","    axs[2].set_ylabel('FPS')\n","    axs[2].set_title('Average Frames Per Second')\n","    axs[2].set_xticks(x)\n","    axs[2].set_xticklabels(models, rotation=15, ha='right')\n","\n","    # Set y-axis limits based on data for FPS\n","    min_fps = 0  #\n","    max_fps = max(avg_fps) * 1.2\n","    axs[2].set_ylim([min_fps, max_fps])\n","\n","    for i, v in enumerate(avg_fps):\n","        axs[2].text(i, v + (max_fps - min_fps) * 0.02, f\"{v:.2f}\", ha='center')\n","\n","    plt.tight_layout()\n","    plt.savefig('resnet18_custom_int8_comparison_avg.png', dpi=300)\n","    plt.show()\n","\n","# Run multiple evaluations and get the average metrics\n","print(\"Starting multiple evaluation runs...\")\n","all_results = run_multiple_evaluations(num_runs=5)\n","print(\"Evaluation complete!\")\n","print(\"Results are saved as 'resnet18_custom_int8_comparison_avg.png'\")\n","\n","print(\"\\nVisualizing sample predictions using models from final run:\")\n","\n","# Load models for visualization\n","resnet18_original = load_model(quantize=False)\n","\n","resnet18_int8_standard = load_model(quantize=True, quantization_type='standard', bits=8)\n","act_quantized_standard = ActivationQuantizer(resnet18_int8_standard, bits=8, use_percentile=False)\n","act_quantized_standard.calibrate(DataLoader(sample_dataset, batch_size=2, shuffle=False))\n","\n","resnet18_int8_percentile = load_model(quantize=True, quantization_type='percentile', bits=8)\n","act_quantized_percentile = ActivationQuantizer(resnet18_int8_percentile, bits=8, use_percentile=True, percentile_value=99.99)\n","act_quantized_percentile.calibrate(DataLoader(sample_dataset, batch_size=2, shuffle=False))\n","\n","print(\"\\nVisualizing sample predictions using Custom INT8 Min-Max Quantized ResNet-18:\")\n","visualize_predictions(act_quantized_standard, sample_dataset, sample_loader, is_quantized=True)\n","\n","print(\"\\nVisualizing sample predictions using Custom INT8 Percentile Quantized ResNet-18:\")\n","visualize_predictions(act_quantized_percentile, sample_dataset, sample_loader, is_quantized=True)\n","\n","print(\"\\nVisualizing sample predictions using original ResNet-18 for comparison:\")\n","visualize_predictions(resnet18_original, sample_dataset, sample_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Zo9MtfyXkaP1SQVJ3g681QwLFoK0mkRU"},"id":"O71mwGP37pNM","executionInfo":{"status":"ok","timestamp":1745268365407,"user_tz":240,"elapsed":172257,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"e0de8b43-834d-43a8-8880-da753dd3aa5b"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}