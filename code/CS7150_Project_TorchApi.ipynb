{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3CFhc3kEBPRO2y2JycV94"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["TORCH INT8 API"],"metadata":{"id":"FiwTDx2cGAZN"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EM2acPy7_GWQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745269923745,"user_tz":240,"elapsed":23730,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"e53171d8-672c-4ef3-9392-84513e692e6a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# PyTorch INT8 API -\n","# This notebook evaluates a pretrained ResNet model with static quantization on ImageNet test data\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import random\n","import os\n","from torch.utils.data import Subset, DataLoader, Dataset\n","from PIL import Image\n","\n","# Import quantization libraries\n","from torch.ao.quantization import get_default_qconfig_mapping, quantize_fx\n","from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","quantization_device = torch.device(\"cpu\")\n","\n","# This is the standard preprocessing for models pretrained on ImageNet\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","imagenet_path = \"/content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/\"\n","print(f\"Loading from: {imagenet_path}\")\n","\n","# Get all jpg files from all folders\n","all_image_files = []\n","class_folders = [f for f in os.listdir(imagenet_path) if os.path.isdir(os.path.join(imagenet_path, f))]\n","print(f\"Found {len(class_folders)} class folders (00000 to 00108)\")\n","\n","# Collect all jpg files\n","for folder in class_folders:\n","    folder_path = os.path.join(imagenet_path, folder)\n","    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.jpg')]\n","    all_image_files.extend(files)\n","\n","print(f\"Found a total of {len(all_image_files)} images\")\n","\n","# Randomly sample 50 images\n","num_samples = 50\n","if len(all_image_files) > num_samples:\n","    random.shuffle(all_image_files)\n","    sampled_images = all_image_files[:num_samples]\n","else:\n","    sampled_images = all_image_files\n","\n","print(f\"Randomly sampled {len(sampled_images)} images for testing\")\n","\n","folder_to_idx = {folder: idx for idx, folder in enumerate(sorted(class_folders))}\n","\n","# Custom dataset for the sampled images\n","class SampledImageNetDataset(Dataset):\n","    def __init__(self, image_files, transform=None):\n","        self.image_files = image_files\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_files[idx]\n","\n","        # Get the class folder from the path\n","        folder_name = os.path.basename(os.path.dirname(img_path))\n","        class_idx = folder_to_idx[folder_name]\n","\n","        # Load and transform the image\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, class_idx\n","\n","# Create the dataset and dataloader\n","sample_dataset = SampledImageNetDataset(sampled_images, transform=preprocess)\n","sample_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False)\n","\n","print(f\"Created dataset with {len(sample_dataset)} images\")\n","print(f\"Number of classes represented: {len(set(folder_to_idx.values()))}\")\n","\n","idx_to_class = {v: k for k, v in folder_to_idx.items()}\n","\n","# Print a few examples\n","print(\"\\nSample images:\")\n","for i in range(min(5, len(sampled_images))):\n","    img_path = sampled_images[i]\n","    folder = os.path.basename(os.path.dirname(img_path))\n","    print(f\"{i+1}. {img_path} (Class: {folder})\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9lUsmjvvrAin","executionInfo":{"status":"ok","timestamp":1745269977861,"user_tz":240,"elapsed":54083,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"b78a4dc8-c23f-47de-a9c9-cbc2fda9d01f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Loading from: /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/\n","Found 109 class folders (00000 to 00108)\n","Found a total of 5450 images\n","Randomly sampled 50 images for testing\n","Created dataset with 50 images\n","Number of classes represented: 109\n","\n","Sample images:\n","1. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00097/9656554087649158.jpg (Class: 00097)\n","2. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00088/938808873819965.jpg (Class: 00088)\n","3. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00025/705343495580209.jpg (Class: 00025)\n","4. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00094/9459481074036453.jpg (Class: 00094)\n","5. /content/drive/My Drive/assignments_dl_cs7150/project/imagenet_data/dataset_dl/00045/811119649116805.jpg (Class: 00045)\n"]}]},{"cell_type":"code","source":["# Function to load ResNet-18 model\n","def load_model(quantize=False):\n","    # Load the PyTorch model\n","    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","    if not quantize:\n","        # Regular non-quantized model\n","        model = model.to(device)\n","        model.eval()\n","        return model\n","\n","    # For static quantization:\n","    # 1. Set up a calibration data loader\n","    calib_dataset = SampledImageNetDataset(sampled_images[:20], transform=preprocess)\n","    calib_loader = DataLoader(calib_dataset, batch_size=1)\n","\n","    # 2. Create a quantization configuration\n","    from torch.ao.quantization import get_default_qconfig\n","\n","    # Get default qconfig\n","    qconfig = get_default_qconfig(\"fbgemm\")\n","    qconfig_dict = {\"\": qconfig}\n","\n","    # Work with the model on CPU for quantization\n","    model = model.to('cpu')\n","    model.eval()\n","\n","    # Create example inputs for prepare_fx\n","    example_inputs = torch.randn(1, 3, 224, 224)\n","\n","    # Prepare the model for quantization\n","    model_prepared = prepare_fx(model, qconfig_dict, example_inputs)\n","\n","    # Calibrate the model with some data\n","    with torch.no_grad():\n","        for inputs, _ in calib_loader:\n","            model_prepared(inputs)\n","\n","    # Convert the model to quantized version\n","    quantized_model = convert_fx(model_prepared)\n","\n","    return quantized_model"],"metadata":{"id":"D4A_7OzQG-uc","executionInfo":{"status":"ok","timestamp":1745269977862,"user_tz":240,"elapsed":1,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Function to measure FPS\n","def measure_fps(model, input_tensor, is_quantized=False):\n","    # For quantized models, ensure we're on CPU\n","    if is_quantized:\n","        input_tensor = input_tensor.to('cpu')\n","        model = model.to('cpu')\n","    else:\n","        input_tensor = input_tensor.to(device)\n","        model = model.to(device)\n","\n","    # Warm-up run\n","    with torch.no_grad():\n","        model(input_tensor)\n","\n","    # Actual timing\n","    start_time = time.time()\n","    num_runs = 10\n","    with torch.no_grad():\n","        for _ in range(num_runs):\n","            model(input_tensor)\n","\n","    end_time = time.time()\n","    time_per_image = (end_time - start_time) / num_runs\n","    fps = 1.0 / time_per_image\n","\n","    return fps\n","\n","# Function to evaluate model on the test set\n","def evaluate_model(model, data_loader, is_quantized=False):\n","    if is_quantized:\n","        model = model.to('cpu')\n","        eval_device = 'cpu'\n","    else:\n","        model = model.to(device)\n","        eval_device = device\n","\n","    correct = 0\n","    total = 0\n","\n","    # For top-1 and top-5 accuracy\n","    top1_correct = 0\n","    top5_correct = 0\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(eval_device), labels.to(eval_device)\n","            outputs = model(images)\n","\n","            # Top-1 accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            top1_correct += (predicted == labels).sum().item()\n","\n","            # Top-5 accuracy\n","            _, top5_preds = torch.topk(outputs, 5, dim=1)\n","            for i in range(labels.size(0)):\n","                if labels[i] in top5_preds[i]:\n","                    top5_correct += 1\n","\n","            total += labels.size(0)\n","\n","    top1_accuracy = 100 * top1_correct / total\n","    top5_accuracy = 100 * top5_correct / total\n","\n","    return top1_accuracy, top5_accuracy"],"metadata":{"id":"_Vq1_b_bG4J0","executionInfo":{"status":"ok","timestamp":1745269977935,"user_tz":240,"elapsed":24,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# See a few sample images with predictions\n","def visualize_predictions(model_obj, dataset, loader, num_samples=5, is_quantized=False):\n","    if is_quantized:\n","        model_obj = model_obj.to('cpu')\n","        vis_device = 'cpu'\n","    else:\n","        model_obj = model_obj.to(device)\n","        vis_device = device\n","\n","    model_obj.eval()\n","    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n","\n","    with torch.no_grad():\n","        for i, (images, labels) in enumerate(loader):\n","            if i >= num_samples:\n","                break\n","\n","            images, labels = images.to(vis_device), labels.to(vis_device)\n","            outputs = model_obj(images)\n","\n","            # Get predicted class\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Get class names (folder names)\n","            true_class_idx = labels.item()\n","            pred_class_idx = predicted.item()\n","\n","            true_class = idx_to_class[true_class_idx]\n","            pred_class = idx_to_class.get(pred_class_idx, f\"Unknown ({pred_class_idx})\")\n","\n","            # Get image\n","            img = images[0].cpu().numpy().transpose((1, 2, 0))\n","            # Denormalize\n","            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n","            img = np.clip(img, 0, 1)\n","\n","            # Plot\n","            axes[i].imshow(img)\n","            axes[i].set_title(f\"True: {true_class}\\nPred: {pred_class}\")\n","            axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig('sample_predictions_quantized.png')\n","    plt.show()\n","\n","\n","# Function to calculate model size in MB\n","def calculate_model_size_mb(model):\n","    \"\"\"Calculate model size in megabytes\"\"\"\n","    # Get model state_dict\n","    state_dict = model.state_dict()\n","\n","    # Calculate size in bytes\n","    total_size = 0\n","    for param in state_dict.values():\n","\n","        if isinstance(param, torch.Tensor):\n","            # Each parameter's size is: num_elements * element_size\n","            num_elements = param.numel()\n","            element_size = param.element_size()  # Size in bytes of each element\n","            total_size += num_elements * element_size\n","\n","    # Convert to MB\n","    size_mb = total_size / (1024 * 1024)\n","    return size_mb\n"],"metadata":{"id":"pKylU2_XG09W","executionInfo":{"status":"ok","timestamp":1745269977957,"user_tz":240,"elapsed":14,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to run multiple evaluations and calculate averages\n","def run_multiple_evaluations(num_runs=5):\n","    # Store results for each run\n","    run_results = {\n","        'models': ['ResNet18 Original', 'ResNet18 INT8 Static'],\n","        'runs': [],\n","        'avg_top1_accuracy': [],\n","        'avg_top5_accuracy': [],\n","        'avg_fps': [],\n","        'avg_model_size_mb': []\n","    }\n","\n","    # Create dummy input for FPS measurement\n","    dummy_input = torch.randn(1, 3, 224, 224)\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Starting Run {run+1}/{num_runs} ---\")\n","        run_data = {\n","            'top1_accuracy': [],\n","            'top5_accuracy': [],\n","            'fps': [],\n","            'model_size_mb': []\n","        }\n","\n","        # Evaluate original PyTorch ResNet-18\n","        print(f\"\\nRun {run+1}: Evaluating original PyTorch ResNet-18...\")\n","        resnet18_original = load_model(quantize=False)\n","        model_size_original = calculate_model_size_mb(resnet18_original)\n","        fps_original = measure_fps(resnet18_original, dummy_input, is_quantized=False)\n","        top1_acc_original, top5_acc_original = evaluate_model(resnet18_original, sample_loader, is_quantized=False)\n","\n","        print(f\"Original model size: {model_size_original:.2f} MB\")\n","\n","        run_data['top1_accuracy'].append(top1_acc_original)\n","        run_data['top5_accuracy'].append(top5_acc_original)\n","        run_data['fps'].append(fps_original)\n","        run_data['model_size_mb'].append(model_size_original)\n","\n","        # Evaluate INT8 static quantized model\n","        print(f\"\\nRun {run+1}: Evaluating ResNet-18 with INT8 Static Quantization...\")\n","        resnet18_int8_static = load_model(quantize=True)\n","        model_size_static = calculate_model_size_mb(resnet18_int8_static)\n","        fps_int8_static = measure_fps(resnet18_int8_static, dummy_input, is_quantized=True)\n","        top1_acc_int8_static, top5_acc_int8_static = evaluate_model(resnet18_int8_static, sample_loader, is_quantized=True)\n","\n","        print(f\"Quantized model size: {model_size_static:.2f} MB\")\n","\n","        run_data['top1_accuracy'].append(top1_acc_int8_static)\n","        run_data['top5_accuracy'].append(top5_acc_int8_static)\n","        run_data['fps'].append(fps_int8_static)\n","        run_data['model_size_mb'].append(model_size_static)\n","\n","        # Store this run's data\n","        run_results['runs'].append(run_data)\n","\n","        # Print a summary of this run\n","        print(f\"\\n--- Run {run+1} Summary ---\")\n","        print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'FPS':<10} {'Size (MB)':<10}\")\n","        print(\"-\" * 85)\n","\n","        for i, model_name in enumerate(run_results['models']):\n","            print(f\"{model_name:<30} {run_data['top1_accuracy'][i]:<15.2f} {run_data['top5_accuracy'][i]:<15.2f} {run_data['fps'][i]:<10.2f} {run_data['model_size_mb'][i]:<10.2f}\")\n","\n","    # Calculate averages across all runs\n","    for metric in ['top1_accuracy', 'top5_accuracy', 'fps']:\n","        for model_idx in range(len(run_results['models'])):\n","            # Calculate average of all runs for each model\n","            values = [run_results['runs'][run_idx][metric][model_idx] for run_idx in range(num_runs)]\n","            run_results[f'avg_{metric}'].append(np.mean(values))\n","\n","    # Calculate improvement/loss metrics\n","    original_model_idx = 0  # Index of the original model in results\n","\n","    # Calculate FPS improvement for static quantized model compared to original\n","    static_model_idx = 1  # Index of the static quantized model\n","    fps_improvement = (run_results['avg_fps'][static_model_idx] / run_results['avg_fps'][original_model_idx] - 1) * 100\n","\n","    # Calculate accuracy loss for static quantized model compared to original\n","    top1_loss = run_results['avg_top1_accuracy'][original_model_idx] - run_results['avg_top1_accuracy'][static_model_idx]\n","    top5_loss = run_results['avg_top5_accuracy'][original_model_idx] - run_results['avg_top5_accuracy'][static_model_idx]\n","\n","    # Print final average results\n","    print(\"\\n\\n====== FINAL RESULTS (AVERAGED OVER 5 RUNS) ======\")\n","    print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'FPS':<10}\")\n","    print(\"-\" * 75)\n","\n","    for i, model_name in enumerate(run_results['models']):\n","        print(f\"{model_name:<30} {run_results['avg_top1_accuracy'][i]:<15.2f} {run_results['avg_top5_accuracy'][i]:<15.2f} {run_results['avg_fps'][i]:<10.2f}\")\n","\n","    # Print improvements/losses\n","    print(\"\\n====== IMPROVEMENTS AND LOSSES ======\")\n","    print(f\"\\n{run_results['models'][1]} compared to Original:\")\n","    print(f\"  FPS Improvement: {fps_improvement:+.2f}%\")\n","    print(f\"  Top-1 Accuracy Loss: {top1_loss:+.2f}%\")\n","    print(f\"  Top-5 Accuracy Loss: {top5_loss:+.2f}%\")\n","\n","    # Plot the results\n","    plot_comparison_results(run_results)\n","\n","    return run_results"],"metadata":{"id":"t1xgc17xGmfd","executionInfo":{"status":"ok","timestamp":1745269977968,"user_tz":240,"elapsed":20,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Function to plot comparison results\n","def plot_comparison_results(results):\n","    # Create a figure with subplots\n","    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n","\n","    # Get the data\n","    models = results['models']\n","    avg_top1 = results['avg_top1_accuracy']\n","    avg_top5 = results['avg_top5_accuracy']\n","    avg_fps = results['avg_fps']\n","    avg_model_size = [results['runs'][0]['model_size_mb'][i] for i in range(len(models))]\n","\n","    # Accuracy plot (combining top-1 and top-5)\n","    bar_width = 0.35\n","    x = np.arange(len(models))\n","\n","    axs[0].bar(x - bar_width/2, avg_top1, bar_width, color='skyblue', label='Top-1')\n","    axs[0].bar(x + bar_width/2, avg_top5, bar_width, color='lightgreen', label='Top-5')\n","    axs[0].set_xlabel('Model')\n","    axs[0].set_ylabel('Accuracy (%)')\n","    axs[0].set_title('Average Test Accuracy')\n","    axs[0].set_xticks(x)\n","    axs[0].set_xticklabels(models)\n","    axs[0].set_ylim([0, 100])\n","    axs[0].legend()\n","    for i, v in enumerate(avg_top1):\n","        axs[0].text(x[i] - bar_width/2, v + 2, f\"{v:.2f}%\", ha='center')\n","    for i, v in enumerate(avg_top5):\n","        axs[0].text(x[i] + bar_width/2, v + 2, f\"{v:.2f}%\", ha='center')\n","\n","    # Model size plot\n","    axs[1].bar(models, avg_model_size, color='salmon')\n","    axs[1].set_xlabel('Model')\n","    axs[1].set_ylabel('Model Size (MB)')\n","    axs[1].set_title('Model Size')\n","    for i, v in enumerate(avg_model_size):\n","        axs[1].text(i, v + 0.5, f\"{v:.2f} MB\", ha='center')\n","\n","    # FPS plot\n","    axs[2].bar(models, avg_fps, color='gold')\n","    axs[2].set_xlabel('Model')\n","    axs[2].set_ylabel('FPS')\n","    axs[2].set_title('Average Frames Per Second')\n","    for i, v in enumerate(avg_fps):\n","        axs[2].text(i, v + 0.5, f\"{v:.2f}\", ha='center')\n","\n","    plt.tight_layout()\n","    plt.savefig('resnet18_pytorch_int8_comparison_avg.png')\n","    plt.show()\n","\n","# Function to run multiple evaluations and calculate averages\n","def run_multiple_evaluations(num_runs=5):\n","    # Store results for each run\n","    run_results = {\n","        'models': ['ResNet18 Original', 'ResNet18 INT8 Static'],\n","        'runs': [],\n","        'avg_top1_accuracy': [],\n","        'avg_top5_accuracy': [],\n","        'avg_fps': [],\n","        'avg_model_size_mb': []\n","    }\n","\n","    # Create dummy input for FPS measurement\n","    dummy_input = torch.randn(1, 3, 224, 224)\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Starting Run {run+1}/{num_runs} ---\")\n","        run_data = {\n","            'top1_accuracy': [],\n","            'top5_accuracy': [],\n","            'fps': [],\n","            'model_size_mb': []\n","        }\n","\n","        # Evaluate original PyTorch ResNet-18\n","        print(f\"\\nRun {run+1}: Evaluating original PyTorch ResNet-18...\")\n","        resnet18_original = load_model(quantize=False)\n","        model_size_original = calculate_model_size_mb(resnet18_original)\n","        fps_original = measure_fps(resnet18_original, dummy_input, is_quantized=False)\n","        top1_acc_original, top5_acc_original = evaluate_model(resnet18_original, sample_loader, is_quantized=False)\n","\n","        print(f\"Original model size: {model_size_original:.2f} MB\")\n","\n","        run_data['top1_accuracy'].append(top1_acc_original)\n","        run_data['top5_accuracy'].append(top5_acc_original)\n","        run_data['fps'].append(fps_original)\n","        run_data['model_size_mb'].append(model_size_original)\n","\n","        # Evaluate INT8 static quantized model\n","        print(f\"\\nRun {run+1}: Evaluating ResNet-18 with INT8 Static Quantization...\")\n","        resnet18_int8_static = load_model(quantize=True)\n","        model_size_static = calculate_model_size_mb(resnet18_int8_static)\n","        fps_int8_static = measure_fps(resnet18_int8_static, dummy_input, is_quantized=True)\n","        top1_acc_int8_static, top5_acc_int8_static = evaluate_model(resnet18_int8_static, sample_loader, is_quantized=True)\n","\n","        print(f\"Quantized model size: {model_size_static:.2f} MB\")\n","\n","        run_data['top1_accuracy'].append(top1_acc_int8_static)\n","        run_data['top5_accuracy'].append(top5_acc_int8_static)\n","        run_data['fps'].append(fps_int8_static)\n","        run_data['model_size_mb'].append(model_size_static)\n","\n","        # Store this run's data\n","        run_results['runs'].append(run_data)\n","\n","        # Print a summary of this run\n","        print(f\"\\n--- Run {run+1} Summary ---\")\n","        print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'FPS':<10} {'Size (MB)':<10}\")\n","        print(\"-\" * 85)\n","\n","        for i, model_name in enumerate(run_results['models']):\n","            print(f\"{model_name:<30} {run_data['top1_accuracy'][i]:<15.2f} {run_data['top5_accuracy'][i]:<15.2f} {run_data['fps'][i]:<10.2f} {run_data['model_size_mb'][i]:<10.2f}\")\n","\n","    # Calculate averages across all runs\n","    for metric in ['top1_accuracy', 'top5_accuracy', 'fps', 'model_size_mb']:\n","        for model_idx in range(len(run_results['models'])):\n","            # Calculate average of all runs for each model\n","            values = [run_results['runs'][run_idx][metric][model_idx] for run_idx in range(num_runs)]\n","            run_results[f'avg_{metric}'].append(np.mean(values))\n","\n","    # Calculate improvement/loss metrics\n","    original_model_idx = 0  # Index of the original model in results\n","\n","    # Calculate FPS improvement for static quantized model compared to original\n","    static_model_idx = 1  # Index of the static quantized model\n","    fps_improvement = (run_results['avg_fps'][static_model_idx] / run_results['avg_fps'][original_model_idx] - 1) * 100\n","\n","    # Calculate model size reduction\n","    size_reduction = (1 - run_results['avg_model_size_mb'][static_model_idx] / run_results['avg_model_size_mb'][original_model_idx]) * 100\n","\n","    # Calculate accuracy loss for static quantized model compared to original\n","    top1_loss = run_results['avg_top1_accuracy'][original_model_idx] - run_results['avg_top1_accuracy'][static_model_idx]\n","    top5_loss = run_results['avg_top5_accuracy'][original_model_idx] - run_results['avg_top5_accuracy'][static_model_idx]\n","\n","    # Print final average results\n","    print(\"\\n\\n====== FINAL RESULTS (AVERAGED OVER 5 RUNS) ======\")\n","    print(f\"{'Model':<30} {'Top-1 Acc (%)':<15} {'Top-5 Acc (%)':<15} {'FPS':<10} {'Size (MB)':<10}\")\n","    print(\"-\" * 85)\n","\n","    for i, model_name in enumerate(run_results['models']):\n","        print(f\"{model_name:<30} {run_results['avg_top1_accuracy'][i]:<15.2f} {run_results['avg_top5_accuracy'][i]:<15.2f} {run_results['avg_fps'][i]:<10.2f} {run_results['avg_model_size_mb'][i]:<10.2f}\")\n","\n","    # Print improvements/losses\n","    print(\"\\n====== IMPROVEMENTS AND LOSSES ======\")\n","    print(f\"\\n{run_results['models'][1]} compared to Original:\")\n","    print(f\"  FPS Improvement: {fps_improvement:+.2f}%\")\n","    print(f\"  Model Size Reduction: {size_reduction:+.2f}%\")\n","    print(f\"  Top-1 Accuracy Loss: {top1_loss:+.2f}%\")\n","    print(f\"  Top-5 Accuracy Loss: {top5_loss:+.2f}%\")\n","\n","    # Plot the results\n","    plot_comparison_results(run_results)\n","\n","    return run_results"],"metadata":{"id":"gUpOv4PKGhzN","executionInfo":{"status":"ok","timestamp":1745269978004,"user_tz":240,"elapsed":51,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# For the final visualization, show predictions using the models from the last run\n","def visualize_final_models():\n","    print(\"\\nVisualizing sample predictions using models from final run:\")\n","\n","    # Load models for visualization\n","    resnet18_original = load_model(quantize=False)\n","    resnet18_int8_static = load_model(quantize=True)\n","\n","    print(\"\\nVisualizing sample predictions using original ResNet-18 for comparison:\")\n","    visualize_predictions(resnet18_original, sample_dataset, sample_loader)\n","\n","    print(\"\\nVisualizing sample predictions using INT8 Static Quantized ResNet-18:\")\n","    visualize_predictions(resnet18_int8_static, sample_dataset, sample_loader, is_quantized=True)\n","\n","# Run multiple evaluations and get the average metrics\n","print(\"Starting multiple evaluation runs...\")\n","all_results = run_multiple_evaluations(num_runs=5)\n","print(\"\\nEvaluation complete!\")\n","print(f\"Results are saved as 'resnet18_pytorch_int8_comparison_avg.png'\")\n","\n","# Visualize predictions using models from the final run\n","visualize_final_models()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fYAZVo_nqN8uyzH8IHeQyLNBxcFdKXv5"},"id":"mpnKqiJ2GeWg","executionInfo":{"status":"ok","timestamp":1745270076760,"user_tz":240,"elapsed":98752,"user":{"displayName":"Poornima JD","userId":"00315572794735409627"}},"outputId":"723a6de3-a242-4632-fbab-2f81c2d6ddb7"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}