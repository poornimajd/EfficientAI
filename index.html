<!doctype html>
<html lang="en">
<head>
    <title>CS7150 - The Quest for Efficient AI</title>
    <meta property="og:title" content="CS7150 - The Quest for Efficient AI" />
    <meta name="twitter:title" content="CS7150 - The Quest for Efficient AI" />
    <meta name="description" content="Your project about your cool topic described right here." />
    <meta property="og:description" content="Your project about your cool topic described right here." />
    <meta name="twitter:description" content="Your project about your cool topic described right here." />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <!-- bootstrap for mobile-friendly layout -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <nobr class="widenobr">CS7150 - The Quest for Efficient AI</nobr>
            </h1>
            <small>- Poornima JD and Archit Bhonsle</small>
        </div>
    </div><!-- end nd-pageheader -->

    <div class="container">
        <div class="row">
            <div class="col">
                <h2>Introduction</h2>
                <p>
                    As deep learning models continue to deliver remarkable performance across a wide range of tasks, deploying them in resource-constrained environments such as mobile devices, edge systems, and autonomous platforms remains a challenge due to their high computational demands, memory usage, and power consumption. This has made it essential to explore strategies that can reduce these overheads while preserving performance.

                    In this context, our project explores how techniques like quantization and knowledge distillation can be effectively used to optimize deep neural networks for deployment in such limited-resource settings.</p>
                <p>

                    Quantization reduces the precision of weights and activations in neural networks from high precision (typically 32-bit floating-point) to lower precision (such as 8-bit integers), significantly reducing memory usage and computational load. For instance, transitioning from 32-bit to 8-bit representations can decrease memory usage by a factor of four and reduce computational costs for matrix operations by up to sixteen-fold. Despite these benefits, quantization introduces approximation errors, potentially degrading model accuracy if not managed carefully.
                </p>
                <p>
                    Knowledge distillation (KD), on the other hand, transfers the predictive capabilities of a large, high-performance "teacher" model into a smaller, more efficient "student" model by training the student to mimic the teacher's "soft" probability outputs rather than traditional "hard" labels. While KD can substantially compress models with minimal accuracy loss, aggressive compression solely via KD often leads to noticeable performance degradation.
                </p>
                    In this work, we experimentally explore and benchmark the practical benefits and trade-offs of these two techniques:<br />

                    <ul>
                        <li>
                            Quantization (Post-Training Quantization - PTQ): We implemented and compared static PTQ both manually and using PyTorch's built-in functions, evaluating the performance differences on CPUs and GPUs.
                        </li>
                        <li>
                            Knowledge Distillation: We distilled knowledge from a larger model to a compact model exclusively on GPU, assessing accuracy retention and efficiency gains.
                        </li>
                    </ul>


                    Our contributions offer a comparative perspective on how PTQ and KD techniques individually impact model efficiency and accuracy, providing empirical insights and practical recommendations for effectively deploying deep neural networks in resource-sensitive environments.
                </p>

                <h2>Explore the Project</h2>
                <p>The project neatly splits into the two paths:</p>
                <ul>
                    <li><a href="part1.html">Part 1: Quantization</a></li>
                    <li><a href="distillation.html">Part 2: Knowledge Distillation</a></li>
                </ul>

                <h2>Team Members</h2>

                <ul>
                    <li>Poornima JD</li>
                    <li>Archit Bhonsle</li>
                </ul>
            </div>
        </div>
    </div>

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://cs7150.baulab.info/">About CS 7150</a>
            </div>
        </div>
    </footer>

</body>
<script>
    $(document).on('click', '.clickselect', function (ev) {
        var range = document.createRange();
        range.selectNodeContents(this);
        var sel = window.getSelection();
        sel.removeAllRanges();
        sel.addRange(range);
    });
    // Google analytics below.
    window.dataLayer = window.dataLayer || [];
</script>
</html>
